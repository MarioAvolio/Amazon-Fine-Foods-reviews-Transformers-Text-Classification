{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarioAvolio/Amazon-Fine-Foods-reviews-Transformers-Text-Classification/blob/main/Text_Preprocessing_Amazon_Fine_Food.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owaHBQRxPfPD"
      },
      "source": [
        "# Credits\n",
        "\n",
        "\n",
        "**Mario Avolio: 880995 - https://marioavolio.netlify.app/**\n",
        "\n",
        "Credits: \n",
        "- https://www.oreilly.com/library/view/practical-natural-language/9781492054047/\n",
        "\n",
        "Dataset:\n",
        "- https://snap.stanford.edu/data/web-FineFoods.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GsRizD7zFJ3V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ6-kXG-0p7V"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeWarxz8ZuPB"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "#https://www.nltk.org/\n",
        "\n",
        "#NLTK is a leading platform for building Python programs to work with human language data. \n",
        "#It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, \n",
        "#along with a suite of text processing libraries for classification, tokenization, \n",
        "#stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength \n",
        "#NLP libraries, and an active discussion forum.\n",
        "\n",
        "nltk.download('punkt') \n",
        "\n",
        "nltk.download('stopwords')\n",
        "# Downloading stop words from NLTK nltk.download ('stopwords')\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "#Tokenizers divide strings into lists of substrings. For example, tokenizers can \n",
        "#be used to find the words and punctuation in a string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfUllriJXH6V"
      },
      "source": [
        "# Constants and Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hhFPP6_GZvC"
      },
      "outputs": [],
      "source": [
        "PATH_PROJ = \"/content/drive/MyDrive/data-proj/\"\n",
        "# if not os.path.exists(PATH_PROJ):\n",
        "#   PATH_PROJ = \"/content/drive/MyDrive/shared/data-proj/\"\n",
        "\n",
        "PATH_DATASET = PATH_PROJ+\"food.csv\"\n",
        "PATH_DATASET_PREPROCESSED = PATH_PROJ+\"preprocessed.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdKztsPGwGat"
      },
      "outputs": [],
      "source": [
        "# Distribution graphs (histogram/bar graph) of column data\n",
        "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
        "    nunique = df.nunique()\n",
        "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
        "    nRow, nCol = df.shape\n",
        "    columnNames = list(df)\n",
        "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
        "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
        "    for i in range(min(nCol, nGraphShown)):\n",
        "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
        "        columnDf = df.iloc[:, i]\n",
        "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
        "            valueCounts = columnDf.value_counts()\n",
        "            valueCounts.plot.bar()\n",
        "        else:\n",
        "            columnDf.hist()\n",
        "        plt.ylabel('Number of sentences')\n",
        "        plt.xticks(rotation = 90)\n",
        "        plt.title(f'{columnNames[i]} (column {i})')\n",
        "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_sentences_from_dataset():\n",
        "  for elem in df.text.sample(30).to_numpy():\n",
        "    print(\" \\n ---> \",elem)\n",
        "  "
      ],
      "metadata": {
        "id": "rdrnVkhfIGgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D98neBGpTn_h"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo_fkDxzTuhY"
      },
      "source": [
        "We would normally\n",
        "walk through the requirements and break the problem down into several subproblems, then try to develop a step-by-step procedure to solve them. Since language\n",
        "processing is involved, we would also list all the forms of text processing needed at\n",
        "each step. This step-by-step processing of text is known as a pipeline. It is the series of\n",
        "steps involved in building any NLP model. These steps are common in every NLP\n",
        "project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK3lpEuNTyjW"
      },
      "source": [
        "The first step in the process of developing any NLP system is to collect data relevant\n",
        "to the given task. Even if we’re building a rule-based system, we still need some data\n",
        "to design and test our rules. The data we get is seldom clean, and this is where text\n",
        "cleaning comes into play. After cleaning, text data often has a lot of variations and\n",
        "needs to be converted into a canonical form. This is done in the pre-processing step.\n",
        "This is followed by feature engineering, where we carve out indicators that are most\n",
        "suitable for the task at hand. These indicators are converted into a format that is\n",
        "understandable by modeling algorithms. Then comes the modeling and evaluation\n",
        "phase, where we build one or more models and compare and contrast them using a\n",
        "relevant evaluation metric(s). Once the best model among the ones evaluated is\n",
        "chosen, we move toward deploying this model in production. Finally, we regularly\n",
        "monitor the performance of the model and, if need be, update it to keep up its\n",
        "performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcr0tZ5tKrtb"
      },
      "source": [
        "# Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJar8wRnGxx5"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(PATH_DATASET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg6Q0J9gHdqq"
      },
      "outputs": [],
      "source": [
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn4JjIG-U28s"
      },
      "outputs": [],
      "source": [
        "print(type(df))\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW6X_tR7-zSj"
      },
      "outputs": [],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izZwz73VvNBz"
      },
      "outputs": [],
      "source": [
        "nRow, nCol = df.shape\n",
        "print(f'There are {nRow} rows and {nCol} columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0leB5LqdYwuD"
      },
      "outputs": [],
      "source": [
        "df.head(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDHROSt0_h7L"
      },
      "source": [
        "Let's isolate the useful columns to our end. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXzoLMTS_hvJ"
      },
      "outputs": [],
      "source": [
        "df = df[[\"text\",\"score\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGYoq1n9Hwgm"
      },
      "outputs": [],
      "source": [
        "df.iloc[30:40]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnLD62LKpEBg"
      },
      "source": [
        "## Looking at the Class Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJH7CXZhpFHD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df[\"score\"].value_counts(ascending=True).plot.barh()\n",
        "plt.title(\"Frequency of Classes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myBZ7TWDpsMS"
      },
      "source": [
        "In this case, we can see that the dataset is heavily imbalanced; There are several ways to deal with imbalanced data, including:\n",
        "- Randomly oversample the minority class.\n",
        "- Randomly undersample the majority class.\n",
        "- Gather more labeled data from the underrepresented classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5fkmgYirZVn"
      },
      "source": [
        "## How Long Are Our Review?\n",
        "Transformer models have a maximum input sequence length that is referred to as the\n",
        "maximum context size. For applications using DistilBERT, the maximum context size\n",
        "is 512 tokens, which amounts to a few paragraphs of text. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIFpYQlpraj8"
      },
      "outputs": [],
      "source": [
        "df[\"Words Per Review\"] = df[\"text\"].str.split().apply(len)\n",
        "df.boxplot(\"Words Per Review\", by=\"score\", grid=False,\n",
        "showfliers=False, color=\"black\")\n",
        "plt.suptitle(\"\")\n",
        "plt.xlabel(\"\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4I5bRyRsUUE"
      },
      "source": [
        "From the plot we see that for each emotion, most tweets are around 60 words long\n",
        "and the longest tweets are well below DistilBERT’s maximum context size. Texts that\n",
        "are longer than a model’s context size need to be truncated, which can lead to a loss in\n",
        "performance if the truncated text contains crucial information; in this case, it looks\n",
        "like that won’t be an issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_5j94dWJLSc"
      },
      "source": [
        "Map target label to String\n",
        "1. VERY NEGATIVE\n",
        "2. NEGATIVE\n",
        "3. NEUTRAL\n",
        "4. POSITIVE\n",
        "5. EXCELLENT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkXA1PGLam3a"
      },
      "source": [
        "# Pre-Processing\n",
        "Our text-extraction step removed all this and gave us the plain text of the\n",
        "article we need. However, all NLP software typically works at the sentence level and\n",
        "expects a separation of words at the minimum. So, we need some way to split a text\n",
        "into words and sentences before proceeding further in a processing pipeline. Some‐\n",
        "times, we need to remove special characters and digits, and sometimes, we don’t care\n",
        "whether a word is in upper or lowercase and want everything in lowercase. Many\n",
        "more decisions like this are made while processing text. Such decisions are addressed\n",
        "during the pre-processing step of the NLP pipeline. Here are some common preprocessing steps used in NLP software:\n",
        "\n",
        "- Preliminaries: Sentence segmentation and word tokenization.\n",
        "- Frequent steps: Stop word removal, stemming and lemmatization, removing digits/punctuation,\n",
        "lowercasing, etc.\n",
        "- Other steps: Normalization, language detection, code mixing, transliteration, etc.\n",
        "- Advanced processing: POS tagging, parsing, coreference resolution, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn4qZJlAK58i"
      },
      "source": [
        "## First Cleanup\n",
        "Text extraction and cleanup refers to the process of extracting raw text from the input\n",
        "data by removing all the other non-textual information, such as markup, metadata,\n",
        "etc., and converting the text to the required encoding format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwmdBm3Z9aMb"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def clean_html_tags(row):\n",
        "  soupified = BeautifulSoup(row, \"html.parser\")\n",
        "  for linebreak in soupified.find_all('br'): #remove br\n",
        "    linebreak.replace_with(\" \")\n",
        "\n",
        "  span_tags = soupified.find_all('span') # remove span\n",
        "  for span in span_tags:\n",
        "      span.unwrap()\n",
        "  \n",
        "  return str(str(soupified))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXq2fWx5D_Kq"
      },
      "outputs": [],
      "source": [
        "df.text.iloc[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl4uZ4QTK8pg"
      },
      "outputs": [],
      "source": [
        "for elem in df.text.sample(30).to_numpy():\n",
        "  print(\" \\n ---> \",clean_html_tags(elem))\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOU3to5i9Biz"
      },
      "outputs": [],
      "source": [
        "df['text'] = df['text'].apply(clean_html_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JEmRsaE8FC9"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ28pslE8QPU"
      },
      "source": [
        "As mentioned earlier, NLP software typically analyzes text by breaking it up into\n",
        "words (tokens) and sentences. Hence, any NLP pipeline has to start with a reliable\n",
        "system to split the text into sentences (sentence segmentation) and further split a sentence into words (word tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKlpszgF8uSg"
      },
      "source": [
        "### Sentence segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMMom7Iy8z3k"
      },
      "source": [
        "As a simple rule, we can do sentence segmentation by breaking up text into sentences\n",
        "at the appearance of full stops and question marks. However, there may be abbrevia‐\n",
        "tions, forms of addresses (Dr., Mr., etc.), or ellipses (...) that may break the simple\n",
        "rule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt-mloAp8GMv"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qLKP25H9tnk"
      },
      "outputs": [],
      "source": [
        "df.text.iloc[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u6-YQd8qLav"
      },
      "outputs": [],
      "source": [
        "sent_tokenize(df.text.iloc[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cvt8685t85C3"
      },
      "outputs": [],
      "source": [
        "df.text = df.text.apply(sent_tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxU0aPPeDGsc"
      },
      "source": [
        "### Word tokenization\n",
        "While readily available solutions work for most of our needs and most NLP libraries\n",
        "will have a tokenizer and sentence splitter bundled with them, it’s important to\n",
        "remember that they’re far from perfect. For example, consider this sentence: “Mr. Jack\n",
        "O’Neil works at Melitas Marg, located at 245 Yonge Avenue, Austin, 70272.” If we run\n",
        "this through the NLTK tokenizer, O, ‘, and Neil are identified as three separate tokens.\n",
        "Similarly, if we run the sentence: “There are \\$10,000 and €1000 which are there just\n",
        "for testing a tokenizer” through this tokenizer, while $ and 10,000 are identified as\n",
        "separate tokens, €1000 is identified as a single token. In another scenario, if we want\n",
        "to tokenize tweets, this tokenizer will separate a hashtag into two tokens: a “#” sign\n",
        "and the string that follows it. In such cases, we may need to use a custom tokenizer\n",
        "built for our purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiQeNnGuF-IF"
      },
      "outputs": [],
      "source": [
        "def word_tokenize_custom(list_of_sent):\n",
        "  list_of_word = []\n",
        "  for sent in list_of_sent:\n",
        "    list_of_word.extend(word_tokenize(sent))\n",
        "\n",
        "  return list_of_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqSCfXTLZZN5"
      },
      "outputs": [],
      "source": [
        "#word_tokenize function\n",
        "\n",
        "print(df.text.iloc[10])\n",
        "print(word_tokenize_custom(df.text.iloc[10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4aX4EEuavdi"
      },
      "source": [
        "When dealing with social media text, we usually want to identify urls, hashtags, smileys as separate objects and do not tokenize it to individual characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFbsdEe1andO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        " \n",
        "emoticons_str = r\"\"\"\n",
        "    (?:\n",
        "        [:=;] # Eyes\n",
        "        [oO\\-]? # Nose (optional)\n",
        "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
        "    )\"\"\"\n",
        " \n",
        "regex_str = [\n",
        "    emoticons_str,\n",
        "    r'<[^>]+>', # HTML tags\n",
        "    r'(?:@[\\w_]+)', # @-mentions\n",
        "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
        "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
        "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
        "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
        "    r'(?:[\\w_]+)', # other words\n",
        "    r'(?:\\S)' # anything else\n",
        "]\n",
        "\n",
        "    \n",
        "\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "\n",
        "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
        "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
        " \n",
        "def tokenize(s):\n",
        "    return tokens_re.findall(s)\n",
        " \n",
        "def preprocess(list_of_sentences):\n",
        "  sentence = []\n",
        "  for sent in list_of_sentences:\n",
        "    sent = emoji_pattern.sub(r'', sent)\n",
        "    sentence.extend(tokenize(sent))\n",
        "  \n",
        "  return np.array(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8udWfSQEmxJ"
      },
      "outputs": [],
      "source": [
        "preprocess(df.text.iloc[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im0x6fvtDI3s"
      },
      "outputs": [],
      "source": [
        "df.text = df.text.apply(preprocess) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUPpoYDGHFev"
      },
      "source": [
        "## Frequent Steps\n",
        "Some of\n",
        "the frequently used words in English, such as a, an, the, of, in, etc., are not particularly useful for this task, as they don’t carry any content on their own to separate\n",
        "between the four categories. Such words are called stop words and are typically\n",
        "(though not always) removed from further analysis in such problem scenarios. There\n",
        "is no standard list of stop words for English, though. There are some popular lists\n",
        "(NLTK has one, for example), although what a stop word is can vary depending on what we’re working on.\n",
        "\n",
        "Similarly, in some cases, upper or lowercase may not make a difference for the problem. So, all text is lowercased (or uppercased, although lowercasing is more common). Removing punctuation and/or numbers is also a common step for many NLP\n",
        "problems, such as text classification, information retrieval,\n",
        "and social media analytics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0U81C-tHH7t"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        " \n",
        "def preprocess_corpus(texts):\n",
        "  '''\n",
        "  Remove stop words, digits, and punctuation and lowercase a given collection of texts\n",
        "  '''\n",
        "  mystopwords = set(stopwords.words(\"english\"))\n",
        "  stop = set()\n",
        "  #adding some of the stopwords after observing the tweets\n",
        "  stop.add(\"The\")\n",
        "  stop.add(\"And\")\n",
        "  stop.add(\"I\")\n",
        "  stop.add(\"J\")\n",
        "  stop.add(\"K\")\n",
        "  stop.add(\"I'd\")\n",
        "  stop.add(\"That's\")\n",
        "  stop.add(\"\\x81\")\n",
        "  stop.add(\"It\")\n",
        "  stop.add(\"I'm\")\n",
        "  stop.add(\"...\")\n",
        "  stop.add(\"\\x89\")\n",
        "  stop.add(\"ĚĄ\")\n",
        "  stop.add(\"it's\")\n",
        "  stop.add(\"ă\")\n",
        "  stop.add(\"\\x9d\")\n",
        "  stop.add(\"âÂĺ\")\n",
        "  stop.add(\"Ě\")\n",
        "  stop.add(\"˘\")\n",
        "  stop.add(\"Â\")\n",
        "  stop.add(\"âÂ\")\n",
        "  stop.add(\"Ň\")\n",
        "  stop.add(\"http\")\n",
        "  stop.add(\"https\")\n",
        "  stop.add(\"co\")\n",
        "  stop.add(\"000\")\n",
        "  stop.add(\"Ň\")\n",
        "  stop.add(\"Ň\")\n",
        "  stop.add(\"Ň\")\n",
        "  stop.add(\"ââ\")\n",
        "  stop.add('ě')\n",
        "  stop.add('ň')\n",
        "  stop.add('``')\n",
        "  stop.add(\"''\")\n",
        "  # stop.add(\"''\")\n",
        "\n",
        "  stop = list(stop)\n",
        "\n",
        "  \n",
        "  def remove_stops_digits(tokens):\n",
        "    return [token.lower() for token in tokens if token not in mystopwords and not token.isdigit() and token not in punctuation and token not in stop]\n",
        "  \n",
        "\n",
        "  return remove_stops_digits(texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSdw6vof6QqU"
      },
      "outputs": [],
      "source": [
        "df.text.iloc[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79de52SlrRhs"
      },
      "outputs": [],
      "source": [
        "preprocess_corpus(df.text.iloc[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJnptpbhP5Xq"
      },
      "outputs": [],
      "source": [
        "df.text = df.text.apply(preprocess_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63K_0S2f7J8w"
      },
      "outputs": [],
      "source": [
        "for elem in df.text.sample(100).to_numpy():\n",
        "  print(\" \\n ---> \",elem)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA6InY3AMxP8"
      },
      "source": [
        "### Stemming and lemmatization\n",
        "\n",
        "Stemming refers to the process of removing suffixes and reducing a word to some\n",
        "base form such that all different variants of that word can be represented by the same form (e.g., “car” and “cars” are both reduced to “car”). This is accomplished by applying a fixed set of rules (e.g., if the word ends in “-es,” remove “-es”). Although such rules may not always end up in a linguistically correct base form, stemming is commonly used in search engines to match user queries to relevant documents and in text classification to reduce the feature space to train machine learning models.\n",
        "\n",
        "\n",
        "Lemmatization is the process of mapping all the different forms of a word to its base\n",
        "word, or lemma. While this seems close to the definition of stemming, they are, in\n",
        "fact, different. For example, the adjective “better,” when stemmed, remains the same.\n",
        "However, upon lemmatization, this should become “good.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DEgr-ToMyZ1"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "def make_stemming(sent):\n",
        "  excluded_words = [\"i've\", \"this\"]\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_sentence = []\n",
        "  for word in sent:\n",
        "    if word in excluded_words:\n",
        "      stemmed_sentence.append(word)\n",
        "      continue\n",
        "\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    stemmed_sentence.append(stemmed_word)\n",
        "  \n",
        "  return stemmed_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYjs8wzQt-DS"
      },
      "outputs": [],
      "source": [
        "df.text.iloc[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQDoPPBut5Ml"
      },
      "outputs": [],
      "source": [
        "make_stemming(df.text.iloc[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFlSHEtFOPhX"
      },
      "outputs": [],
      "source": [
        "# df.text = df.text.apply(make_stemming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmxfCyVz9kAg"
      },
      "outputs": [],
      "source": [
        "# get_random_sentences_from_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lgPdC1MPb0c"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "def make_lemmatization(sentence):\n",
        "  sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "  list_of_lemmatize_words = []\n",
        "  for word in sentence:\n",
        "    token = sp(word) # The ‘u’ in front of a string means the string is a Unicode string.\n",
        "    list_of_lemmatize_words.append(token[0].lemma_)\n",
        "  \n",
        "  return list_of_lemmatize_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxS97fT8vSJE"
      },
      "outputs": [],
      "source": [
        "df.text.iloc[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xi7b-3vMwkso"
      },
      "outputs": [],
      "source": [
        "make_lemmatization(df.text.iloc[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h79MCrjxR-RW"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "tqdm.pandas(desc=\"progress: \")\n",
        "# df.text = df.text.progress_apply(make_lemmatization) # too much expensive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dictionary check"
      ],
      "metadata": {
        "id": "VT1i_EYCFjpp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH7gI7a09N4X"
      },
      "outputs": [],
      "source": [
        "nltk.download('words')\n",
        "from nltk.corpus import words\n",
        "\n",
        "def return_real_word(sentence):\n",
        "  return [w for w in sentence if w in words.words()]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(return_real_word(df.text.iloc[10])), len(df.text.iloc[10])"
      ],
      "metadata": {
        "id": "qzITkQlOGJ4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas(desc=\"progress: \")\n",
        "\n",
        "# df.text = df['text'].progress_apply(return_real_word) # too much expensive"
      ],
      "metadata": {
        "id": "Qqm4eCEyG_D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove single token"
      ],
      "metadata": {
        "id": "s12DDW6tI9ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_single_token(sentence):\n",
        "  return [word for word in sentence if len(word)>1]"
      ],
      "metadata": {
        "id": "f20sbRkLH0Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(remove_single_token(df.text.iloc[2])), len(df.text.iloc[2])"
      ],
      "metadata": {
        "id": "uStNchXRJejo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.text.iloc[2]"
      ],
      "metadata": {
        "id": "iRfJWHFEJhFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.text = df.text.apply(remove_single_token) # too much expensive"
      ],
      "metadata": {
        "id": "QJ1IilYzL_K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_random_sentences_from_dataset()"
      ],
      "metadata": {
        "id": "o62lKxL7Ou_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RxPp9Irtig0"
      },
      "outputs": [],
      "source": [
        "df.to_csv(PATH_DATASET_PREPROCESSED, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBbaMD0mnh4U"
      },
      "source": [
        "# Visualize words"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iCvFwavUaVtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02SMkLZynYeV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx_oKZqennFl"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the counter (will be helpful later on)\n",
        "def get_counter(series):\n",
        "  flat_list = [item for sublist in series for item in sublist]\n",
        "  c = Counter(flat_list)\n",
        "  return c"
      ],
      "metadata": {
        "id": "3QGam8EjaW6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMfFhbksnrhc"
      },
      "outputs": [],
      "source": [
        "flat_list = [item for sublist in df.text for item in sublist] #unique list containing all tokens \n",
        "  \n",
        "fig = plt.figure(figsize=(20,14))\n",
        "wordcloud = WordCloud(width=1600, height=800, background_color=\"black\").generate_from_frequencies(Counter(flat_list))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(wordcloud, interpolation='antialiased')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_counter(df.text).most_common(10)"
      ],
      "metadata": {
        "id": "H1DgWcbsabl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "11TZf-A0ac15"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}